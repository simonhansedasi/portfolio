{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f879989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glacierml as gl\n",
    "import path_manager as pm\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "pd.set_option('display.max_column',None)\n",
    "\n",
    "[\n",
    "        home_path, data_path, RGI_path, glathida_path, \n",
    "        ref_path, coregistration_testing_path, \n",
    "        arch_test_path, LOO_path\n",
    "] = pm.set_paths()\n",
    "\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c032f8e8-2b23-4dd0-8ba0-c1823cc359b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "#### Let's do a calculation.\n",
    "#### ${S} \\approx \\sum_{k=1}^{N_k} \\hat{V}_  k \\pm Z^*_{\\alpha / 2} \\sqrt{\\sum_k^{N_k} \\sigma^2_k}$\n",
    "\n",
    "#### First, let's load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57f582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols, df = gl.load_LOO_data(home_path, include_train = False, include_refs = False)\n",
    "cols, train = gl.load_LOO_data(home_path, include_train = True, include_refs = False)\n",
    "\n",
    "g = (\n",
    "    pd.read_csv(os.path.join(glathida_path, 'T.csv'))[\n",
    "        ['AREA','MEAN_THICKNESS_UNCERTAINTY','MEAN_THICKNESS']\n",
    "    ]\n",
    ")\n",
    "g = g.rename(columns = {\n",
    "    'MEAN_THICKNESS_UNCERTAINTY':'u',\n",
    "    'AREA':'a',\n",
    "    'MEAN_THICKNESS':'t'\n",
    "})\n",
    "g.u = g.u/1e3\n",
    "g.t = g.t/1e3\n",
    "g = g.dropna(subset = ['u'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea45879b",
   "metadata": {},
   "source": [
    "#### For the case of independent variables, $\\sigma^2_k =  \\left(\\sigma^H_k\\right)^2 \\left(\\sigma^A_k\\right)^2 + {A}_k^2 \\left(\\sigma^H_k\\right)^2 \n",
    "     +  H_k^2 \\left(\\sigma^A_k\\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ef55a",
   "metadata": {},
   "source": [
    "#### Let $\\left(\\sigma^H_k\\right)^2 = \\text{Var}\\left(\\epsilon^{\\mathcal{H}}_k\\right) + \\text{Var}\\left(\\epsilon^{\\mathcal{R}}_k\\right) +\\text{Var}\\left(\\epsilon^{\\mathcal{M}}_k\\right)$,\n",
    "\n",
    "<!-- % #### and $\\text{Var}\\left(\\epsilon^{\\mathcal{H}}_k\\right) \\approx \\text{Var}_j\\left(\\mathcal{H}_{kj}\\right)$, -->\n",
    "\n",
    "#### and $\\text{Var}\\left(\\epsilon^{\\mathcal{H}}_k\\right) \\approx \\text{Var}_j\\left(\\mathcal{H}_{kj}\\right)$,\n",
    "\n",
    "<!-- #### and $\\text{Var}\\left(\\epsilon^{\\mathcal{R}}_B\\right) \\approx \\frac{1}{n_B n_j}\\sum_B^{n_B}\\sum_{j}^{n_j} \\left(r_{jB} - \\mu_{r_B}\\right)^2$, -->\n",
    "\n",
    "#### and $\\text{Var}\\left(\\epsilon^{\\mathcal{R}}_B\\right) \\approx \\text{Var}_j \\left(\\mathcal{R}_{jB}\\right)$,\n",
    "\n",
    "#### and $\\text{Var}\\left(\\epsilon^{\\mathcal{M}}_i\\right) \\approx \\text{Var} \\left(\\mathcal{M}_{i}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77d9230",
   "metadata": {},
   "source": [
    "#### Start with $\\text{Var}\\left(\\epsilon^{\\mathcal{H}}_k\\right)\\approx \\text{Var}_j\\left(\\mathcal{H}_{kj}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f10943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### epsilonH is the uncertainty due to limited training data ###\n",
    "### var(epsilonH) represents variance of  ###\n",
    "# var_eps_H = (1/(len(cols) - 1)*np.sum(\n",
    "#     df[cols].sub(np.mean(df[cols],axis = 1),axis = 0)**2,axis = 1\n",
    "# )\n",
    "var_eps_H = np.var(df[cols],axis = 1)\n",
    "df = pd.concat([df,pd.Series(var_eps_H,name = 'var_eps_H')],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61edc9c5",
   "metadata": {},
   "source": [
    "#### That was easy. Now for  $\\text{Var}\\left(\\epsilon^{\\mathcal{R}}_B\\right) \\approx \\text{Var}_j \\left(\\mathcal{R}_{jB}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9986058",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate Var(epsR_B) ###\n",
    "### First bin residuals ###\n",
    "bins = []\n",
    "[bins.append(x/1e3) for x in range(1, 300, 1)]\n",
    "bins.append(0)\n",
    "bins = np.sort(bins).tolist()\n",
    "names = [str(x) for x in bins]\n",
    "bins.append(np.inf)\n",
    "train = pd.concat(\n",
    "    [\n",
    "        train,\n",
    "         pd.cut(train['Thickness'], bins, labels=names).rename('bin')\n",
    "    ],axis = 1\n",
    ")\n",
    "train['bin'] = train['bin'].astype(float)\n",
    "bins = train['bin'].sort_values().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece18b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Iterate through residuals and calculate a standard deviation ###\n",
    "c = dict.fromkeys(bins)\n",
    "re = dict.fromkeys(bins)\n",
    "\n",
    "for n,i in tqdm(enumerate(c.keys())):    \n",
    "    if i <= 170/1e3:\n",
    "        j = i + 1/1e3\n",
    "    if i > 170/1e3:\n",
    "        j = i - 10/1e3\n",
    "    dft = train[\n",
    "        (train['bin'] == i)       \n",
    "               ]\n",
    "    while len(dft) < 2:\n",
    "        if i <= 95/1e3:\n",
    "            j +=1/1e3\n",
    "            dft = train[\n",
    "            (train['bin'] >= i)&\n",
    "            (train['bin'] <= j)       \n",
    "                   ]\n",
    "        if i >= 95/1e3 and i < 170/1e3:\n",
    "            j +=10/1e3\n",
    "            dft = train[\n",
    "            (train['bin'] >= i)&\n",
    "            (train['bin'] <= j)       \n",
    "                   ]\n",
    "        if i > 170/1e3:\n",
    "            j -=10/1e3\n",
    "            dft = train[\n",
    "            (train['bin'] <= i)&\n",
    "            (train['bin'] >= j)       \n",
    "                   ]\n",
    "        \n",
    "    h = dft[cols]\n",
    "    r = h.subtract(dft['Thickness'], axis=0)\n",
    "    c[i] = np.std(r.to_numpy().flatten())\n",
    "    re[i] = np.mean(r.to_numpy(), axis=0) \n",
    "stds = np.fromiter(c.values(), dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4afc9d",
   "metadata": {},
   "source": [
    "#### Now we can fit $\\sqrt{\\text{Var}_j \\left(\\mathcal{R}_{jB}\\right)}$ to $h_B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08491dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit a statistical model to the range \n",
    "z = np.linspace(0.01,0.3,99) \n",
    "# z = train['Thickness']\n",
    "model = np.poly1d(np.polyfit(np.log(z),np.log(stds), 1))\n",
    "\n",
    "c1 = np.round(np.exp(model[0]),2)\n",
    "c2 = np.round((model[1]),1)\n",
    "print(c1)\n",
    "print(c2)\n",
    "\n",
    "\n",
    "plt.scatter((z),(stds),alpha = 0.25,label = r'$\\sigma^{\\mathcal{R}}_B$')\n",
    "plt.plot((z),np.exp(model(np.log(z))),\n",
    "                    label = rf'$\\sigma^{{\\mathcal{{R}}}}_B \\approx {c1} h_B^{{{c2}}} $',\n",
    "        c = 'orange'\n",
    "        )\n",
    "plt.legend()\n",
    "plt.title(r'Standard Deviation of Binned Residuals $\\sigma^{\\mathcal{R}}_B$')\n",
    "plt.xlabel('GlaThida Thickness Bin (km)')\n",
    "plt.ylabel(r'$\\sigma^{\\mathcal{R}}_B$ (km)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb60492e",
   "metadata": {},
   "source": [
    "#### Apply to $H_k$ and we're done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply statistical model of residuals to mean estimated thickness ###\n",
    "var_eps_R = (np.mean((c1*df[cols]**c2),axis = 1))**2\n",
    "df = pd.concat([df,pd.Series(var_eps_R,name = 'var_eps_R')],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db8a8e",
   "metadata": {},
   "source": [
    "#### now for $\\text{Var}\\left(\\epsilon^{\\mathcal{M}}_m\\right) \\approx \\text{Var}\\left(\\mathcal{M}_{m}\\right)$. \n",
    "\n",
    "#### GlaThiDa reports a thickness uncertainty. Consider this a std dev $\\sigma^{\\mathcal{M}}_m$ where $n_m < n_i$. We can do as we did with variance of residuals and fit another statistical model.\n",
    "\n",
    "#### Fit a a statistical model with $\\sqrt{\\text{Var}\\left(\\mathcal{M}_{m}\\right)}$ as the dependent variable to corresponding measured thickness $h_m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = []\n",
    "for i in g.u:\n",
    "    g_u = gl.findlog(i)\n",
    "    u.append(g_u)\n",
    "u = np.array(u)\n",
    "### Fit a statistical model to the range \n",
    "# z = np.linspace(g.a.min(),g.a.max(),len(g)) \n",
    "# z = train['Thickness']\n",
    "model = np.poly1d(np.polyfit(np.log(g.t),(u), 1))\n",
    "\n",
    "c3 = np.round(np.exp(model[0]),2)\n",
    "c4 = np.round((model[1]),1)\n",
    "print(c3)\n",
    "print(c4)\n",
    "\n",
    "\n",
    "plt.scatter((g.t),(g.u),alpha = 0.25,label = r'$\\sigma^{\\mathcal{M}}_m$')\n",
    "plt.plot(np.sort(g.t),np.exp(model(np.log(np.sort(g.t)))),\n",
    "                    label = rf'$\\sigma^{{\\mathcal{{M}}}}_m \\approx {c3} h_m^{{{c4}}} $',\n",
    "         c = 'orange'\n",
    "        )\n",
    "plt.legend()\n",
    "# plt.title(r'Standard Deviation of Binned Residuals $\\sigma^{\\mathcal{R}}_B$')\n",
    "plt.xlabel('GlaThida Measured Thickness for which we have uncertainty $h_m$ (km)')\n",
    "plt.ylabel(r'GlaThiDa Reported Thickness Uncertainty $\\sigma^{\\mathcal{M}}_m$ (km)')\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "plt.grid(which = 'both')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2901d10f",
   "metadata": {},
   "source": [
    "#### Apply to $H_k$ and we're done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa14927",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_eps_M = (np.mean((c3*df[cols]**c4),axis = 1))**2\n",
    "df = pd.concat([df,pd.Series(var_eps_M,name = 'var_eps_M')],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9670d",
   "metadata": {},
   "source": [
    "#### Okay, thinking about residuals. We have captured the variance of residuals, but we haven't really considered the accuracy of our residuals Var($\\epsilon^{\\mathcal{RA}}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f1c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put them together ###\n",
    "var_H = var_eps_H + var_eps_R + var_eps_M\n",
    "df = pd.concat([df,pd.Series(var_H,name = 'var_H')],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.mean(df[cols],axis = 1)\n",
    "# xy = np.vstack([np.log10(h),np.log10(var_eps_H)])\n",
    "# print('calculating density')\n",
    "# z = gaussian_kde(xy)(xy)\n",
    "# np.save('epsh_z.npy',z)\n",
    "# z = np.load('epsh_z.npy')\n",
    "# plt.scatter(h,var_eps_H,label = 'h')\n",
    "plt.scatter(h,(var_eps_H),label = r'Var$\\left(\\epsilon^H_k\\right)$',marker = '.')\n",
    "\n",
    "plt.scatter(h,(var_eps_R),label = r'Var$\\left(\\epsilon^R_k\\right)$',marker = '.')\n",
    "plt.scatter(h,var_eps_M,label = r'Var$\\left(\\epsilon^M_k\\right)$',marker = '.')\n",
    "plt.xlabel('Estimated Mean Thickness (km)')\n",
    "plt.ylabel('(km)$^2$')\n",
    "plt.title('Global Glacier Thickness Uncertainty Components')\n",
    "plt.legend()\n",
    "# plt.scatter(np.mean(df[cols],axis = 1),var_eps_H)\n",
    "# plt.scatter(np.mean(df[cols],axis = 1),var_eps_H + var_eps_R + var_eps_M)\n",
    "# plt.scatter(np.mean(df[cols],axis = 1),np.mean(df[cols],axis = 1))\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64830cb6",
   "metadata": {},
   "source": [
    "#### Okay, we have $(\\sigma^H_k)^2$ calculated and appended.\n",
    "#### Let's calculate $(\\sigma^A_k)^2$. We follow the procedure set out by Pfeffer et al. 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "p = 0.7\n",
    "e1 = 0.039\n",
    "var_A = (k*e1*(df['Area']**p))**2\n",
    "df = pd.concat([df,pd.Series(var_A,name = 'var_A')],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8d639",
   "metadata": {},
   "source": [
    "#### That was easy. Now what is left?\n",
    "#### Let $H_k = \\frac{1}{N_j}\\sum_{j=1}^{N_j} \\mathcal{H}_{kj}$, and $A_k$ be the reported RGI Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8c30de",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculation for variance of independent variables. ###\n",
    "E_A = df['Area']\n",
    "E_H = np.mean(df[cols],axis = 1)\n",
    "\n",
    "v1 = (var_H * var_A)\n",
    "v2 = (var_H * E_A**2) \n",
    "v3 = (E_H**2 * var_A)\n",
    "sigma_k_ind = v1 + v2 + v3 \n",
    "df = pd.concat([df,pd.Series(sigma_k_ind,name = 'sig_k_ind')],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1366c",
   "metadata": {},
   "source": [
    "#### ${S} \\approx \\sum_{k=1}^{N_k} \\hat{V}_k \\pm Z^*_{\\alpha / 2} \\sqrt{\\sum_k^{N_k} \\sigma^2_k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "si = df['sig_k_ind'] # = (Var(H)*Var(A) + A^2Var(H) + H^2Var(A))\n",
    "\n",
    "sp = np.sum(si)\n",
    "\n",
    "se = np.sqrt(sp)\n",
    "\n",
    "Z = (1.96)\n",
    "Vlb = ( (np.sum(E_H * E_A) - (Z*se)) ) / 1e3\n",
    "Vub = ( (np.sum(E_H * E_A) + (Z*se)) ) / 1e3\n",
    "\n",
    "print(f'[{Vlb},{Vub}]  * 10^3 km^3')\n",
    "\n",
    "print(f'Mid CI: {((Vub + Vlb) / 2)} * 10^3 km^3')\n",
    "\n",
    "print(f'CI Half Width: {np.round((Vub - Vlb) / 2,3)} * 10^3 km^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "x = E_H\n",
    "y = (np.sqrt(var_H) / (E_H)) * 100\n",
    "# if not os.path.exists('thickness_z.npy'):\n",
    "#     xy = np.vstack([np.log10(x),np.log10(y)])\n",
    "#     print('calculating density')\n",
    "#     z = gaussian_kde(xy)(xy)\n",
    "#     np.save(z,'thickness_z.npy')\n",
    "# else:\n",
    "#     z = np.load('thickness_z.npy')\n",
    "ax[0].scatter(E_H, (np.sqrt(var_H) / (E_H)) * 100, marker='.',\n",
    "              label=r'$\\sigma_k / H_k \\times 100$')\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_ylabel('Glacier Thickness Percent Uncertainty')\n",
    "ax[0].set_xlabel('Glacier Thickness (km)')\n",
    "\n",
    "x = E_H * E_A\n",
    "y = (np.sqrt(df['sig_k_ind']) / (E_H * E_A)) * 100\n",
    "# if not os.path.exists('thickness_z.npy'):\n",
    "#     xy = np.vstack([np.log10(x),np.log10(y)])\n",
    "#     print('calculating density')\n",
    "#     z = gaussian_kde(xy)(xy)\n",
    "#     np.save(z,'vol_z.npy')\n",
    "# else:\n",
    "#     z = np.load('vol_z.npy')\n",
    "ax[1].scatter(x, y, marker='.')\n",
    "\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_ylabel('Glacier Volume Percent Uncertainty')\n",
    "ax[1].set_xlabel('Glacier Volume (km$^3$)')\n",
    "\n",
    "fig.suptitle('Global Thickness and Volume Percent Uncertainties')\n",
    "\n",
    "# Add labels 'A' and 'B' to upper-right corners of the axes\n",
    "ax[0].text(0.95, 0.95, 'A', transform=ax[0].transAxes, fontsize=12, ha='right', va='top', fontweight='bold')\n",
    "ax[1].text(0.95, 0.95, 'B', transform=ax[1].transAxes, fontsize=12, ha='right', va='top', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4334efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize = (8,4))\n",
    "\n",
    "# ax[0].legend()\n",
    "\n",
    "# ax[0].set_yscale('log')\n",
    "# ax[0].set_title('Global Glacier Volume Percent Uncertainty')\n",
    "\n",
    "X = E_H\n",
    "Y = np.sqrt(var_H)\n",
    "\n",
    "bins_X = np.logspace(np.log10(np.min(X)), np.log10(np.max(X)), 25)\n",
    "bins_Y = np.logspace(np.log10(np.min(Y)), np.log10(np.max(Y)), 25)\n",
    "\n",
    "# fig,ax = plt.subplots(1,2,figsize = (12,5))\n",
    "ax[1].hist(X,bins = bins_X,alpha = 0.5,log = True,label = r'${H}_k$')\n",
    "ax[1].hist(Y,bins = bins_Y,alpha = 0.5,log = True,label = r'${\\sigma_k}$')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_xlabel('Glacier Thickness (km)')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "fig.suptitle('Global Glacier Thickness Uncertainty',y = 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f4db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize = (8,4))\n",
    "\n",
    "ax[0].scatter(E_H,((var_H)/(E_H)),marker = '.',\n",
    "              label = r'$\\sigma_k^2 / H_k $')\n",
    "ax[0].set_xscale('log')\n",
    "# plt.yscale('log')\n",
    "ax[0].set_ylabel('Glacier Thickness Coefficient of Variation')\n",
    "ax[0].set_xlabel('Glacier Thickness (km)')\n",
    "ax[0].legend()\n",
    "              \n",
    "# ax[0].set_yscale('log')\n",
    "# ax[0].set_title('Global Glacier Volume Percent Uncertainty')\n",
    "\n",
    "X = E_H\n",
    "Y = (var_H) / E_H\n",
    "\n",
    "bins_X = np.logspace(np.log10(np.min(X)), np.log10(np.max(X)), 25)\n",
    "bins_Y = np.logspace(np.log10(np.min(Y)), np.log10(np.max(Y)), 25)\n",
    "\n",
    "# fig,ax = plt.subplots(1,2,figsize = (12,5))\n",
    "ax[1].hist(X,bins = bins_X,alpha = 0.5,log = True,label = r'${H}_k$')\n",
    "ax[1].hist(Y,bins = bins_Y,alpha = 0.5,log = True,label = r'${{\\sigma_k}^2}/{\\mu_k}$')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_xlabel('Glacier Thickness (km$^3$)')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "fig.suptitle('Global Glacier Thickness Uncertainty',y = 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58107352",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize = (8,4))\n",
    "\n",
    "ax[0].scatter(E_A,((var_H)/(E_H)),marker = '.',\n",
    "              label = r'$\\sigma_k^2 / H_k $')\n",
    "ax[0].set_xscale('log')\n",
    "# plt.yscale('log')\n",
    "ax[0].set_ylabel('Glacier Thickness Coefficient of Variation')\n",
    "ax[0].set_xlabel('Glacier Thickness (km)')\n",
    "ax[0].legend()\n",
    "              \n",
    "# ax[0].set_yscale('log')\n",
    "# ax[0].set_title('Global Glacier Volume Percent Uncertainty')\n",
    "\n",
    "X = E_H\n",
    "Y = (var_H) / E_H\n",
    "\n",
    "bins_X = np.logspace(np.log10(np.min(X)), np.log10(np.max(X)), 25)\n",
    "bins_Y = np.logspace(np.log10(np.min(Y)), np.log10(np.max(Y)), 25)\n",
    "\n",
    "# fig,ax = plt.subplots(1,2,figsize = (12,5))\n",
    "ax[1].hist(X,bins = bins_X,alpha = 0.5,log = True,label = r'${H}_k$')\n",
    "ax[1].hist(Y,bins = bins_Y,alpha = 0.5,log = True,label = r'${{\\sigma_k}^2}/{\\mu_k}$')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_xlabel('Glacier Thickness (km$^3$)')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "fig.suptitle('Global Glacier Thickness Uncertainty',y = 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = np.mean(df[cols],axis = 1) * df['Area']\n",
    "data_2 = np.sqrt(df['sig_k_ind'])\n",
    "\n",
    "ks_stat, p_value = stats.ks_2samp(data_1, data_2)\n",
    "print(ks_stat)\n",
    "# data_2 = data['FMT'] * data['Area']\n",
    "#sort data\n",
    "\n",
    "fig = plt.subplots(1,1,figsize = (10,8))\n",
    "cdf_data_1 = np.sort(data_1)\n",
    "cdf_data_2 = np.sort(data_2)\n",
    "\n",
    "\n",
    "cdf1 = np.arange(len(data_1)) / float(len(data_1))\n",
    "cdf2 = np.arange(len(data_2)) / float(len(data_2))\n",
    "\n",
    "\n",
    "plt.plot(cdf_data_1, cdf1, label='RGI Volume')\n",
    "plt.plot(cdf_data_2, cdf2, label='RGI Volume Uncertainty')\n",
    "\n",
    "plt.title('CDF of Glacier Volume and Volume Uncertainty Ignoring Covariance')\n",
    "plt.xlabel('Value (km$^3$)')\n",
    "plt.ylabel('CDF')\n",
    "plt.legend(loc = 'center right')\n",
    "plt.xscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543c24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('final.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722946d6",
   "metadata": {},
   "source": [
    "#### And that's it for this notebook. For a further breakdown of variance components see variance_analysis.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glacierml (Python3.8.10)",
   "language": "python",
   "name": "glacierml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
